# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_lit_model.ipynb.


__all__ = [
    # "Scheduler",
    # "CosineLRScheduler",
    # "plot_lr_step_schedule",
    # "_lr_func_linear_warmup_by_epoch",
    # "_lr_function_by_epoch",
    # "_get_scheduler",
    "print_example",
    "AbstractLitModel",
    # "_lr_function_by_step",
]

import os
from datetime import datetime, timedelta
from pytorch_lightning.utilities.types import STEP_OUTPUT
from torch.optim.lr_scheduler import LambdaLR

import torch
import torch.nn as nn
from fastcore.all import *

from loguru import logger
from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
from pytorch_lightning.callbacks.progress import TQDMProgressBar
from pytorch_lightning.loggers import TensorBoardLogger

from ple.schedulers import get_cyclic_cosine_lr

try:
    from ple.loss import BinaryFocalLoss, FocalLoss
except Exception as e:
    FocalLoss, BinaryFocalLoss = None, None
import os.path as osp

from typing import Any


def print_example():
    str = """
#---- HYPER
EPOCHS = 30
LR = 1e-4
BZ = 8
GPUS = 1
NUM_WORKERS = 4            
STRATEGY = 'dp'
# --- INIT dataset
train_ds = #
val_ds = #

dl_train = torch.utils.data.DataLoader(train_ds, BZ, num_workers=NUM_WORKERS, shuffle=True)
dl_val = torch.utils.data.DataLoader(train_ds, BZ, num_workers=NUM_WORKERS, shuffle=False)
# ---- Lr scheduler
sched = fn_schedule_cosine_with_warmpup_decay_timm(
    num_epochs=EPOCHS,
    num_steps_per_epoch=len(dl_train),
    num_epochs_per_cycle=EPOCHS//2,
    min_lr=1/100,
    cycle_decay=0.7,
)
# --- Optimizer
optim = lambda params:torch.optim.Adam(params, lr=LR)

# ---- LitModel
class CustomLit(LitModel):
    def forward(self, batch):
        # Get loss

    def training_step(self, batch, idx):
        out = self.forward(batch)
        return out['loss']

    def validation_step(self, batch, idx):
        out = self.forward(batch)
        self.log('val/loss', out['loss'], prog_bar=True, on_epoch=True)

lit = CustomLit(model,create_optimizer_fn=optim,
                               create_lr_scheduler_fn=sched, loss_fn=nn.CrossEntropyLoss())    

#---------------- Train

trainer = get_trainer('trans_lit', EPOCHS, gpus=GPUS, overfit_batches=0.05,
                     monitor={'metric': 'val/loss', 'mode': 'min'}, strategy=STRATEGY)
trainer.fit(lit, dl_train, dl_val)
    """
    print(str)


import time


class AbstractLitModel(LightningModule):
    def __init__(
        self,
        model,
        loss_fn=nn.CrossEntropyLoss(),
        optim=None,
        num_epochs=None,
        grad_accumulate_steps=None,
        train_loader=None
    ):
        super().__init__()
        self.model = model
        self.loss_fn = loss_fn
        self.optim_func = None
        self.grad_accumulate_steps = grad_accumulate_steps
        self.optim = optim
        self.num_epochs = num_epochs
        self.train_loader = train_loader

    def forward(self, x):
        return self.model(x)

    def configure_optimizers(self):
        optimizer = self.optim

        # Assuming the scheduler is step-based, and we want to step it every grad accumulation step.
        if (
            optimizer is None
            or self.num_epochs is None
            or self.grad_accumulate_steps is None
        ):
            logger.info("Not enough infor to init schduler")
            return [optimizer]

        lr_func_by_step = get_cyclic_cosine_lr(
            self.train_loader, self.num_epochs, self.grad_accumulate_steps
        )
        scheduler = {
            "scheduler": LambdaLR(optimizer, lr_func_by_step, -1),
            "interval": "step",  # step-based update
            "frequency": self.grad_accumulate_steps,
        }

        return [optimizer], [scheduler]

    def compute_predictions(self, logits):
        return logits.softmax(1).argmax(1)

    def log_metrics(self, metrics, prog_bar=True, on_epoch=True):
        for key, value in metrics.items():
            self.log(
                key, value, prog_bar=prog_bar, rank_zero_only=True, on_epoch=on_epoch
            )

    def training_step(self, batch, batch_idx):
        pass

    def validation_step(self, *args: Any, **kwargs: Any):
        pass
    
    # def log(self, *args, **kwargs):