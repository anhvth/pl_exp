# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_base_exp.ipynb.

# %% auto 0
__all__ = ['BaseExp']

# %% ../nbs/04_base_exp.ipynb 2
import ast
import pprint
from abc import ABCMeta, abstractmethod
from typing import Dict
from tabulate import tabulate

import torch
from torch.nn import Module
import pytorch_lightning as pl
from torch.optim.lr_scheduler import _LRScheduler
from .all import get_trainer
import os.path as osp

class BaseExp(metaclass=ABCMeta):
    """Basic class for any experiment."""

    def __init__(self):
        self.accelerator = 'cpu'
        
    @abstractmethod
    def get_model(self) -> Module:
        pass

    @abstractmethod
    def get_data_loader(
        self,
    ) -> pl.LightningDataModule:
        pass

    @abstractmethod
    def get_optimizer() -> torch.optim.Optimizer:
        pass

    @abstractmethod
    def get_lr_scheduler(
        self, lr: float, iters_per_epoch: int, **kwargs
    ) -> _LRScheduler:
        pass

    def __repr__(self):
        table_header = ["keys", "values"]
        exp_table = [
            (str(k), pprint.pformat(v))
            for k, v in vars(self).items()
            if not k.startswith("_")
        ]
        return tabulate(exp_table, headers=table_header, tablefmt="fancy_grid")
    def merge(self, cfg_list):
        assert len(cfg_list) % 2 == 0
        for k, v in zip(cfg_list[0::2], cfg_list[1::2]):
            # only update value with same key
            if hasattr(self, k):
                src_value = getattr(self, k)
                src_type = type(src_value)
                if src_value is not None and src_type != type(v):
                    try:
                        v = src_type(v)
                    except Exception:
                        v = ast.literal_eval(v)
                setattr(self, k, v)
    @property
    def exp_name(self):
        if hasattr(self, '_exp_name'): # 
            return self._exp_name
        else:
            exp_name = osp.basename(__file__).split('.')[0]
            print(f'Autogenerated {exp_name=}')
            return exp_name
        
    def get_trainer(self, devices:int):
        return get_trainer(self.exp_name,
                               devices,
                              max_epochs=self.max_epochs, 
                              trainer_kwargs=dict(
                                  accelerator=self.accelerator,
                              )
                            )
