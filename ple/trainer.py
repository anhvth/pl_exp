# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_trainer.ipynb.


from fastcore.utils import *
from fastcore.script import *
import shutil
import os

__all__ = ["is_interactive", "get_trainer", "get_rank", "get_exp_by_file", "train"]


import os.path as osp

from loguru import logger
from pytorch_lightning import Trainer
import torch
from .custom_callbacks import *
# from .schedulers import 

def is_interactive():
    return "get_ipython" in dir()


def get_trainer(
    exp_name=None,
    max_epochs=None,
    num_gpus=1,
    monitor=dict(metric="val_loss", mode="min"),
    strategy="auto",
    accelerator="gpu",
    refresh_rate=10,
    save_kwargs={},
    save_kwargs_default={
        "save_last": True,
        "save_top_k": -1,
        "every_n_epochs": 1,
        "every_n_train_steps": None,
    },
    find_unused_parameters=True,
    **kwargs,
) -> Trainer:
    save_kwargs_default.update(save_kwargs)
    if not torch.cuda.is_available():
        num_gpus = 1
        accelerator = "cpu"
        logger.warning("No GPU available, using CPU instead, gpus=1, accelerator='cpu'")

    callbacks = []
    plt_logger = None
    if exp_name is not None:
        rld = osp.join("lightning_logs", exp_name)
        cur_num_exps = len(os.listdir(rld)) if osp.exists(rld) else 0
        version = f"{cur_num_exps:02d}"
        root_log_dir = osp.join("lightning_logs", exp_name, version)
        if monitor is None:
            filename = None
        else:
            filename = "{epoch}_{" + monitor["metric"] + ":0.4f}"

        callback_ckpt = ModelCheckpoint(
            dirpath=osp.join(root_log_dir),
            monitor=monitor["metric"] if monitor is not None else None,
            mode=monitor["mode"] if monitor is not None else "min",
            filename=filename,
            **save_kwargs_default,
        )

        plt_logger = CustomTensorBoardLogger(
            osp.join(root_log_dir),
        )
        callbacks.append(callback_ckpt)

    callbacks.append(TQDMProgressBar(refresh_rate=refresh_rate))
    callbacks.append(LearningRateMonitor(logging_interval="step"))
    callbacks.append(ThroughputLogger())
    if strategy is None:
        if is_interactive():
            logger.info("gpus={}, Interactive mode, force strategy=auto", num_gpus)
            strategy = "auto"
        elif num_gpus < 2:
            logger.info("gpus={}, , force strategy=dp", num_gpus)
            strategy = "auto"
        else:
            strategy = "ddp"
            logger.info("gpus={}, strategy=ddp, set strategy='dp' if this", num_gpus)
    if "strategy" == "ddp":
        from pytorch_lightning.strategies.ddp import DDPStrategy

        strategy = DDPStrategy(find_unused_parameters=find_unused_parameters)

    trainer = Trainer(
        accelerator=accelerator,
        devices=num_gpus,
        max_epochs=max_epochs,
        strategy=strategy,
        callbacks=callbacks,
        log_every_n_steps=refresh_rate,
        logger=plt_logger,
        **kwargs,
    )
    return trainer


# # from argparse import ArgumentParser
# from pytorch_lightning import Trainer, seed_everything
# # from ple.all import get_trainer, BaseExp


def get_rank() -> int:
    import torch.distributed as dist

    if not dist.is_available():
        return 0
    if not dist.is_initialized():
        return 0
    return dist.get_rank()


def get_exp_by_file(exp_file):
    """
    Params:
    exp_file: Path to exp

    """
    try:
        import importlib
        import os
        import sys

        sys.path.append(os.path.dirname(exp_file))
        # import ipdb; ipdb.set_trace()
        current_exp = importlib.import_module(os.path.basename(exp_file).split(".")[0])
        current_exp = importlib.reload(current_exp)
        exp = current_exp.Exp()
        return exp
    except Exception:
        raise ImportError("{} doesn't contains class named 'Exp'".format(exp_file))


@call_parse
def train(
    cfg_path: Param("Path to config"),
    devices: Param("GPUS indices", default=1, type=int),
    opts: Param("Additional configs", default="", type=str, required=False),
):
    cfg = get_exp_by_file(cfg_path)
    if len(opts):
        cfg.merge(opts.replace("=", " ").split(" "))
    cfg.devices = devices

    data = cfg.get_data_loader()

    model = cfg.get_model(
        create_lr_scheduler_fn=cfg.get_lr_scheduler(),
        create_optimizer_fn=cfg.get_optimizer(),
    )
    trainer = cfg.get_trainer(devices)
    try:
        trainer.fit(model, data)
    except Exception as e:
        import traceback

        traceback.print_exc()
    finally:
        if get_rank() == 0:
            out_path = osp.join(trainer.log_dir, osp.basename(cfg_path))
            logger.info("cp {} {}", cfg_path, out_path)
            shutil.copy(cfg_path, out_path)
