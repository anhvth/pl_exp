{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2c017-7c7a-48b0-ac7f-8c19c7074e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34991d1-2a84-4202-a436-f840e06e2166",
   "metadata": {},
   "source": [
    "# LitModel\n",
    "> Lit model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69641cf-7889-43c6-89e7-1cde9d864950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37a026-d840-4e4e-a449-ee5e38698326",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525de612-6478-4a7a-b7e0-59c949076681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from loguru import logger\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from lit_classifier.loss import FocalLoss, BinaryFocalLoss\n",
    "import os.path as osp\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41a5af-319e-43de-ab38-f225f65d596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isinstance(torch.optim.Adam(model.parameters()), torch.optim.Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90006936-9100-40da-978e-49f205108e3b",
   "metadata": {},
   "source": [
    "## Get optim cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e3565-c6d0-462d-9517-ffff75a6fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "def get_optim_cfg(epochs, steps_per_ep, lr=1e-3, init_lr=0.5, min_lr=0.2, interval='step', optim='Adam'):\n",
    "    steps = epochs*steps_per_ep\n",
    "    return dict(lr=lr, init_lr=init_lr, min_lr=min_lr, steps=steps, epochs=epochs, interval=interval, optim=optim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798d5f4-7773-4273-95df-8969cf3ebe12",
   "metadata": {},
   "source": [
    "## Lit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3160b-5f11-4ccd-906e-d8d8b584cf96",
   "metadata": {},
   "source": [
    "## Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9add50-1b56-42f9-8feb-4a4a56d53e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_linear_scheduler(optimizer, optim_cfg):\n",
    "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, init_lr, min_lr, num_epochs, interval):\n",
    "        def lr_lambda(current_step: int):\n",
    "            if current_step < num_warmup_steps:\n",
    "                x = (1-init_lr)*(current_step / num_warmup_steps)+init_lr\n",
    "                return x\n",
    "            if interval=='epoch':\n",
    "                steps_per_ep = num_training_steps / num_epochs\n",
    "                current_ep = current_step // steps_per_ep\n",
    "                current_step = steps_per_ep*current_ep\n",
    "\n",
    "            total_step = (num_training_steps-num_warmup_steps)\n",
    "            current_step = current_step-num_warmup_steps\n",
    "            rt = min_lr+(1-min_lr)*(1-current_step/total_step)\n",
    "            return rt\n",
    "        return LambdaLR(optimizer, lr_lambda, -1)\n",
    "    \n",
    "    scheduler = {\n",
    "        \"scheduler\": get_linear_schedule_with_warmup(optimizer,\n",
    "            optim_cfg[\"steps\"] * 0.15,\n",
    "            optim_cfg[\"steps\"],\n",
    "            optim_cfg[\"init_lr\"],\n",
    "            optim_cfg[\"min_lr\"],\n",
    "            optim_cfg[\"epochs\"],\n",
    "            optim_cfg['interval']\n",
    "        ),\n",
    "        \"interval\": 'step',  # or 'epoch'\n",
    "        \"frequency\": 1,\n",
    "    }\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2cdb5f-da74-4702-a4ee-802c67d8bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LitModel(LightningModule):\n",
    "    def __init__(self, model, optim_cfg=None, loss=FocalLoss(), scheduler_fn=get_linear_scheduler):\n",
    "        \"\"\"\n",
    "            Params:\n",
    "                scheduler_fn: function to create schedualer, (self.scheduler_fn(optimizer))\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss\n",
    "        self.optim_cfg = optim_cfg\n",
    "        self.lr = 10e-3\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "            Setup optimizer and scheduler\n",
    "        \"\"\"\n",
    "        if self.optim_cfg is None:\n",
    "            logger.warning('Please add optim cfg and re-init this object')\n",
    "            return\n",
    "        if self.optim_cfg['optim'] == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.optim_cfg[\"lr\"])\n",
    "        elif self.optim_cfg['optim'] == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=self.optim_cfg[\"lr\"], \n",
    "                                          betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "        else:\n",
    "            assert isinstance(self.optim_cfg['optim'], torch.optim.Optimizer) \n",
    "            optimizer = self.optim_cfg['optim']\n",
    "        # if self.optim_cfg['']\n",
    "        # if self.scheduler_fn is None:\n",
    "        #     scheduler = get_linear_scheduler(optimizer, self.optim_cfg)\n",
    "        # else:\n",
    "        scheduler = dict(\n",
    "            scheduler=self.scheduler_fn(optimizer, optim_cfg),\n",
    "            interval=\"step\",\n",
    "            frequency=1,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        logits = self(x)\n",
    "        scores = logits.sigmoid()\n",
    "        # return dict(scores=scores)\n",
    "        return scores\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = logits.softmax(1).argmax(1)\n",
    "        accs = (y == preds).float().mean()\n",
    "\n",
    "\n",
    "        self.log(\"val_loss\", loss, rank_zero_only=True, prog_bar=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", accs, rank_zero_only=True, prog_bar=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = logits.softmax(1).argmax(1)\n",
    "        accs = (y == preds).float().mean()\n",
    "        \n",
    "        self.log(\"training_loss\", loss, prog_bar=True, rank_zero_only=True, on_epoch=True)\n",
    "        self.log(\"training_accuracy\", accs, prog_bar=True, rank_zero_only=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BinLitModel(LitModel):\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x).reshape(-1)\n",
    "        y = y.reshape(logits.shape)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = logits.sigmoid() > 0.5\n",
    "        accs = (y == preds).float().mean()\n",
    "\n",
    "\n",
    "        self.log(\"val_loss\", loss, rank_zero_only=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", accs, rank_zero_only=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x).reshape(-1)\n",
    "        y = y.reshape(logits.shape)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = logits.sigmoid() > 0.5\n",
    "        accs = (y == preds).float().mean()\n",
    "        \n",
    "        self.log(\"training_loss\", loss, prog_bar=True, rank_zero_only=True)\n",
    "        self.log(\"training_accuracy\", accs, prog_bar=True, rank_zero_only=True)\n",
    "        return loss\n",
    "    \n",
    "# from lit_classifier.lit_model import LitModel\n",
    "# PLitModel = persistent_class(LitModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb7f2b-973b-444b-b88f-91d46c63f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_lit_state_dict(ckpt_path):\n",
    "    st = torch.load(ckpt_path)['state_dict']\n",
    "    out_st = {}\n",
    "    for k, v in st.items():\n",
    "        out_st[k.replace('model.', '')] = v\n",
    "    return out_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c685b6e-4adc-47e2-9f0a-fd86e5bb5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import timm, matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "optim_cfg = get_optim_cfg(10, 1000, interval='step', lr=0.01)\n",
    "model = timm.create_model('resnet18')\n",
    "model = LitModel(model)\n",
    "# [optim], [sche] = model.configure_optimizers()\n",
    "# sche = sche['scheduler']\n",
    "# lrs = []\n",
    "# for i in range(10000):\n",
    "#     lrs.append(sche.get_lr())\n",
    "#     sche.step()\n",
    "# plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b955898-c203-4380-9e7c-d5390ce37213",
   "metadata": {},
   "source": [
    "## Get trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31867c0d-0d51-4e22-b9e5-54c7a04e99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_trainer(exp_name, gpus=1, max_epochs=None, distributed=False,\n",
    "        monitor=dict(metric=\"val_acc\", mode=\"max\"), save_every_n_epochs=1, save_top_k=1, use_version=True,\n",
    "    trainer_kwargs=dict(), optim_cfg=None):\n",
    "    if max_epochs is None:\n",
    "        assert optim_cfg is not None, f'optim_cfg and max_epoch cannot be both None'\n",
    "        max_epochs = optim_cfg['max_epoch']\n",
    "        \n",
    "    now = datetime.now() + timedelta(hours=7)\n",
    "    \n",
    "    root_log_dir = osp.join(\n",
    "            \"lightning_logs\", exp_name)\n",
    "    cur_num_exps = len(os.listdir(root_log_dir)) if osp.exists(root_log_dir) else 0\n",
    "    version = now.strftime(f\"{cur_num_exps:02d}_%b%d_%H_%M\")\n",
    "    if use_version:\n",
    "        root_log_dir = osp.join(root_log_dir, version)\n",
    "        logger.info('Root log directory: {}'.format(root_log_dir))\n",
    "    filename=\"{epoch}-{\"+monitor[\"metric\"]+\":.2f}\"\n",
    "\n",
    "    callback_ckpt = ModelCheckpoint(\n",
    "        dirpath=osp.join(root_log_dir, \"ckpts\"),\n",
    "        monitor=monitor['metric'],mode=monitor['mode'],\n",
    "        filename=filename,\n",
    "        save_last=True,\n",
    "        every_n_epochs=save_every_n_epochs,\n",
    "        save_top_k=save_top_k,\n",
    "    )\n",
    "\n",
    "    callback_tqdm = TQDMProgressBar(refresh_rate=5)\n",
    "    callback_lrmornitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    plt_logger = TensorBoardLogger(\n",
    "        osp.join(root_log_dir, \"tb_logs\"), version=version\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        gpus=gpus,\n",
    "        max_epochs=max_epochs,\n",
    "        strategy= \"dp\" if not distributed else \"ddp\",\n",
    "        callbacks=[callback_ckpt, callback_tqdm, callback_lrmornitor],\n",
    "        logger=plt_logger,**trainer_kwargs,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba2a23-a15c-4e67-af10-855e8c556e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nbdev_build_lib\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ee2f3-7c66-4a6d-8ef3-d2900ffab513",
   "metadata": {},
   "source": [
    "# Mnist example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f882e-fd65-4f24-8277-f87e13724232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "BATCH_SIZE = 256 #if AVAIL_GPUS else 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2fd93-09d7-4603-84b1-c4377de5899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.transforms_factory import transforms_imagenet_train, transforms_imagenet_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f440a0-cc14-4b6a-876e-a57346444b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "def to_rgb(x):\n",
    "    return x.convert('RGB')\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(to_rgb),\n",
    "        *transforms_imagenet_train(32).transforms,\n",
    "    ]\n",
    ")\n",
    "class ClassifierDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.mnist_test = MNIST(self.data_dir, train=False, transform=train_transform)\n",
    "        self.mnist_predict = MNIST(self.data_dir, train=False, transform=train_transform)\n",
    "        mnist_full = MNIST(self.data_dir, train=True, transform=train_transform)\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n",
    "\n",
    "    # def teardown(self, stage: Optional[str] = None):\n",
    "        # Used to clean-up when the run is finished\n",
    "# mnist = MNISTDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc450cd-be66-432d-91ec-05105588ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_classifier.persistance import persistent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff5c68-3ebf-49f3-8991-3e370051c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init DataLoader from MNIST Dataset\n",
    "from timm.data.transforms_factory import transforms_imagenet_train, transforms_imagenet_eval\n",
    "\n",
    "T = transforms.Compose(\n",
    "[    \n",
    "    transforms.Lambda(to_rgb),\n",
    "    *transforms_imagenet_eval(32).transforms,\n",
    "]\n",
    ")\n",
    "PMNIST = persistent_class(MNIST)\n",
    "train_ds = PMNIST(root='./', train=True, download=True, transform=T)\n",
    "test_ds = PMNIST(root='./', train=False, download=True, transform=T)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE*2, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9f8eb-cfc1-48d6-a246-eb4d34b1fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce68b9-5a6a-4f82-b366-11fb9681dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 15:24:08.150 | INFO     | lit_classifier.lit_model:get_trainer:176 - Root log directory: lightning_logs/test/01_Jul30_22_24\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lit_classifier import base_model\n",
    "from lit_classifier.lit_model import *\n",
    "from lit_classifier.loss import FocalLoss\n",
    "import timm\n",
    "# model = base_model.model_factory('mobilenetv2_035', 10, pretrained=False)\n",
    "model = timm.create_model('resnet18', True, num_classes=10)\n",
    "optim_cfg = get_optim_cfg(10, len(train_loader), lr=1e-4, optim='AdamW')\n",
    "lit_model = LitModel(model, optim_cfg, loss=FocalLoss())\n",
    "trainer = get_trainer('test', gpus=[0], distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41388061-48b3-4357-9600-653f9db99bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac48dc6-689e-4be5-b149-cc6632b65820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7f6ec613d940>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0b965-c8b4-415d-aefa-77a433fa4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lit_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9489e66-d370-49d1-9ac7-afa6767a145c",
   "metadata": {},
   "source": [
    "# BUILD LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07610a0c-d60f-48af-b337-ed407b318caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ac7ed-a0be-41e4-8905-1bbcfc0f31bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35afd161-3287-433a-8ff9-2c6833bd1059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b441f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a9bc6-ed25-4421-912b-0aad04466241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
