{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7e5be9-9401-40e3-9cdd-7212e169b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp base_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74423bcc-7f8c-41b7-8aad-230996e7313d",
   "metadata": {},
   "source": [
    "# Base_exp\n",
    "| Base_exp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3b3acb-65c4-4ee2-a608-ea9e4a7f5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import ast\n",
    "import pprint\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from typing import Dict\n",
    "from tabulate import tabulate\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "class BaseExp(metaclass=ABCMeta):\n",
    "    \"\"\"Basic class for any experiment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # All hyper params should be listed here\n",
    "        self.accelerator = 'gpu'\n",
    "        self.max_epochs:int = 100\n",
    "        self.devices:int = 1\n",
    "        \n",
    "        self.schedule_type = 'cosine'\n",
    "        self.num_lr_cycles = 3\n",
    "        self.min_lr = 1/100\n",
    "        self.cycle_decay = 0.7\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_model(self) -> Module:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "    ) -> pl.LightningDataModule:\n",
    "        pass\n",
    "\n",
    "    def get_optimizer(self) -> torch.optim.Optimizer:\n",
    "         return lambda params:torch.optim.Adam(params)\n",
    "\n",
    "\n",
    "    def get_lr_scheduler(self, train_loader_len=None):\n",
    "        if train_loader_len is None:\n",
    "            data = self.get_data_loader()\n",
    "            data.setup(None)\n",
    "            #-------\n",
    "            num_epochs = self.max_epochs\n",
    "            train_loader = data.train_dataloader()\n",
    "            train_loader_len = len(train_loader)\n",
    "        \n",
    "        if self.schedule_type == 'cosine':\n",
    "            from ple.lit_model import fn_schedule_cosine_with_warmpup_decay_timm\n",
    "            create_schedule_fn = fn_schedule_cosine_with_warmpup_decay_timm(\n",
    "                num_epochs=self.max_epochs,\n",
    "                num_steps_per_epoch=train_loader_len//self.devices,\n",
    "                num_epochs_per_cycle=self.max_epochs//self.num_lr_cycles,\n",
    "                min_lr=self.min_lr,\n",
    "                cycle_decay=self.cycle_decay,\n",
    "            )\n",
    "        elif self.schedule_type == 'linear':\n",
    "            from ple.lit_model import fn_schedule_linear_with_warmup\n",
    "            create_schedule_fn = fn_schedule_linear_with_warmup(\n",
    "                num_epochs=self.trainer.max_epochs,\n",
    "                num_steps_per_epoch=train_loader_len//self.devices\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return create_schedule_fn\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        table_header = [\"keys\", \"values\"]\n",
    "        exp_table = [\n",
    "            (str(k), pprint.pformat(v))\n",
    "            for k, v in vars(self).items()\n",
    "            if not k.startswith(\"_\")\n",
    "        ]\n",
    "        return tabulate(exp_table, headers=table_header, tablefmt=\"fancy_grid\")\n",
    "    \n",
    "    def merge(self, cfg_list):\n",
    "        assert len(cfg_list) % 2 == 0\n",
    "        for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n",
    "            # only update value with same key\n",
    "            if hasattr(self, k):\n",
    "                src_value = getattr(self, k)\n",
    "                src_type = type(src_value)\n",
    "                if src_value is not None and src_type != type(v):\n",
    "                    try:\n",
    "                        v = src_type(v)\n",
    "                    except Exception:\n",
    "                        v = ast.literal_eval(v)\n",
    "                print(f'Set {k}={v}')\n",
    "                setattr(self, k, v)\n",
    "        \n",
    "    def get_trainer(self):\n",
    "        from ple.trainer import get_trainer\n",
    "        return get_trainer(self.exp_name, \n",
    "                              max_epochs=self.max_epochs, \n",
    "                              gpus=self.devices,\n",
    "                              trainer_kwargs=dict(\n",
    "                                  accelerator=self.accelerator,\n",
    "                              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95bc3db0-5289-4d06-8478-b2fc401a6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8548048b-e5fb-4f4f-b2d2-b3bed026aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15c3e8-fc5c-4bc7-87f2-2e40a0a00b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
