{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e5be9-9401-40e3-9cdd-7212e169b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp base_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74423bcc-7f8c-41b7-8aad-230996e7313d",
   "metadata": {},
   "source": [
    "# Base_exp\n",
    "| Base_exp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b3acb-65c4-4ee2-a608-ea9e4a7f5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import ast\n",
    "import pprint\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from typing import Dict\n",
    "from tabulate import tabulate\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "class BaseExp(metaclass=ABCMeta):\n",
    "    \"\"\"Basic class for any experiment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # All hyper params should be listed here\n",
    "        self.accelerator = 'gpu'\n",
    "        self.max_epochs:int = 100\n",
    "        self.devices:int = 1\n",
    "        \n",
    "        self.schedule_type = 'cosine'\n",
    "        self.num_lr_cycles = 3\n",
    "        self.min_lr = 1/100\n",
    "        self.cycle_decay = 0.7\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_model(self) -> Module:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "    ) -> pl.LightningDataModule:\n",
    "        pass\n",
    "\n",
    "    def get_optimizer(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "            Examples:\n",
    "                return lambda params:torch.optim.Adam(params)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_lr_scheduler(self, train_loader_len=None):\n",
    "        if train_loader_len is None:\n",
    "            data = self.get_data_loader()\n",
    "            data.setup(None)\n",
    "            #-------\n",
    "            num_epochs = self.max_epochs\n",
    "            train_loader = data.train_dataloader()\n",
    "            train_loader_len = len(train_loader)\n",
    "        \n",
    "        if self.schedule_type == 'cosine':\n",
    "            from ple.lit_model import fn_schedule_cosine_with_warmpup_decay_timm\n",
    "            create_schedule_fn = fn_schedule_cosine_with_warmpup_decay_timm(\n",
    "                num_epochs=self.max_epochs,\n",
    "                num_steps_per_epoch=train_loader_len//self.devices,\n",
    "                num_epochs_per_cycle=self.max_epochs//self.num_lr_cycles,\n",
    "                min_lr=self.min_lr,\n",
    "                cycle_decay=self.cycle_decay,\n",
    "            )\n",
    "        elif self.schedule_type == 'linear':\n",
    "            from ple.lit_model import fn_schedule_linear_with_warmup\n",
    "            create_schedule_fn = fn_schedule_linear_with_warmup(\n",
    "                num_epochs=self.trainer.max_epochs,\n",
    "                num_steps_per_epoch=train_loader_len//self.devices\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return create_schedule_fn\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        table_header = [\"keys\", \"values\"]\n",
    "        exp_table = [\n",
    "            (str(k), pprint.pformat(v))\n",
    "            for k, v in vars(self).items()\n",
    "            if not k.startswith(\"_\")\n",
    "        ]\n",
    "        return tabulate(exp_table, headers=table_header, tablefmt=\"fancy_grid\")\n",
    "    \n",
    "    def merge(self, cfg_list):\n",
    "        assert len(cfg_list) % 2 == 0\n",
    "        for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n",
    "            # only update value with same key\n",
    "            if hasattr(self, k):\n",
    "                src_value = getattr(self, k)\n",
    "                src_type = type(src_value)\n",
    "                if src_value is not None and src_type != type(v):\n",
    "                    try:\n",
    "                        v = src_type(v)\n",
    "                    except Exception:\n",
    "                        v = ast.literal_eval(v)\n",
    "                print(f'Set {k}={v}')\n",
    "                setattr(self, k, v)\n",
    "        \n",
    "    def get_trainer(self):\n",
    "        from ple.trainer import get_trainer\n",
    "        return get_trainer(self.exp_name, \n",
    "                              max_epochs=self.max_epochs, \n",
    "                              gpus=self.devices,\n",
    "                              trainer_kwargs=dict(\n",
    "                                  accelerator=self.accelerator,\n",
    "                              ))\n",
    "    def plot_lr_sche(self):\n",
    "        data = self.get_data_loader()\n",
    "        step_per_epoch = len(data.train_dataloader())\n",
    "        sched = self.get_lr_scheduler(step_per_epoch)\n",
    "        plot_lr_step_schedule(sched, self.lr, self.epochs, step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57508600-44de-41ff-8056-051d6a028dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤══════════╕\n",
      "│ keys          │ values   │\n",
      "╞═══════════════╪══════════╡\n",
      "│ accelerator   │ 'gpu'    │\n",
      "├───────────────┼──────────┤\n",
      "│ max_epochs    │ 100      │\n",
      "├───────────────┼──────────┤\n",
      "│ devices       │ 1        │\n",
      "├───────────────┼──────────┤\n",
      "│ schedule_type │ 'cosine' │\n",
      "├───────────────┼──────────┤\n",
      "│ num_lr_cycles │ 3        │\n",
      "├───────────────┼──────────┤\n",
      "│ min_lr        │ 0.01     │\n",
      "├───────────────┼──────────┤\n",
      "│ cycle_decay   │ 0.7      │\n",
      "╘═══════════════╧══════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 11:40:09.542 | INFO     | ple.lit_model:fn_schedule_cosine_with_warmpup_decay_timm:65 - num_cycles=3\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "class MyExp(BaseExp):\n",
    "    def get_model():return None\n",
    "    def get_data_loader(): return None\n",
    "exp = MyExp()\n",
    "print(exp)\n",
    "sched = exp.get_lr_scheduler(1000)\n",
    "# from ple.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e47d46-b35d-4590-9d94-4f28dad13a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = exp.get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc3db0-5289-4d06-8478-b2fc401a6df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15c3e8-fc5c-4bc7-87f2-2e40a0a00b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0ad2f-6d0b-4e8a-8c1a-b9f3ee6d93d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b03c86-d93e-4536-b913-54e9cdd19348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfbe6f-9ebb-49f4-8a5a-575115b3b450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620d044-abb7-46ed-8d61-2d0b5704e7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
