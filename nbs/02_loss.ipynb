{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c25c9-b230-458c-b682-35405b3d825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08976c-0923-48d8-8d31-c915ceeb4318",
   "metadata": {},
   "source": [
    "# Losses\n",
    "> Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d7467",
   "metadata": {},
   "source": [
    "# Focalloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87185d-78e7-41b0-b053-9946025ef43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import functools\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    \"\"\"Reduce loss as specified.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Elementwise loss tensor.\n",
    "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
    "\n",
    "    Return:\n",
    "        Tensor: Reduced loss tensor.\n",
    "    \"\"\"\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, elementwise_mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()\n",
    "\n",
    "\n",
    "def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):\n",
    "    \"\"\"Apply element-wise weight and reduce loss.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Element-wise loss.\n",
    "        weight (Tensor): Element-wise weights.\n",
    "        reduction (str): Same as built-in losses of PyTorch.\n",
    "        avg_factor (float): Average factor when computing the mean of losses.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Processed loss values.\n",
    "    \"\"\"\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    else:\n",
    "        # if reduction is mean, then average the loss by avg_factor\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.sum() / avg_factor\n",
    "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
    "        elif reduction != 'none':\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss\n",
    "\n",
    "# from ..builder import LOSSES\n",
    "\n",
    "\n",
    "# This method is only for debugging\n",
    "def py_sigmoid_focal_loss(pred,\n",
    "                          target,\n",
    "                          weight=None,\n",
    "                          gamma=2.0,\n",
    "                          alpha=0.25,\n",
    "                          reduction='mean',\n",
    "                          avg_factor=None):\n",
    "    \"\"\"PyTorch version of `Focal Loss <https://arxiv.org/abs/1708.02002>`_.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, C), C is the\n",
    "            number of classes\n",
    "        target (torch.Tensor): The learning label of the prediction.\n",
    "        weight (torch.Tensor, optional): Sample-wise loss weight.\n",
    "        gamma (float, optional): The gamma for calculating the modulating\n",
    "            factor. Defaults to 2.0.\n",
    "        alpha (float, optional): A balanced form for Focal Loss.\n",
    "            Defaults to 0.25.\n",
    "        reduction (str, optional): The method used to reduce the loss into\n",
    "            a scalar. Defaults to 'mean'.\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "    \"\"\"\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    target = target.type_as(pred)\n",
    "    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "    focal_weight = (alpha * target + (1 - alpha) *\n",
    "                    (1 - target)) * pt.pow(gamma)\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, target, reduction='none') * focal_weight\n",
    "    if weight is not None:\n",
    "        if weight.shape != loss.shape:\n",
    "            if weight.size(0) == loss.size(0):\n",
    "                # For most cases, weight is of shape (num_priors, ),\n",
    "                #  which means it does not have the second axis num_class\n",
    "                weight = weight.view(-1, 1)\n",
    "            else:\n",
    "                # Sometimes, weight per anchor per class is also needed. e.g.\n",
    "                #  in FSAF. But it may be flattened of shape\n",
    "                #  (num_priors x num_class, ), while loss is still of shape\n",
    "                #  (num_priors, num_class).\n",
    "                assert weight.numel() == loss.numel()\n",
    "                weight = weight.view(loss.size(0), -1)\n",
    "        assert weight.ndim == loss.ndim\n",
    "    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(pred,\n",
    "                       target,\n",
    "                       weight=None,\n",
    "                       gamma=2.0,\n",
    "                       alpha=0.25,\n",
    "                       reduction='mean',\n",
    "                       avg_factor=None):\n",
    "    r\"\"\"A warpper of cuda version `Focal Loss\n",
    "    <https://arxiv.org/abs/1708.02002>`_.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n",
    "            of classes.\n",
    "        target (torch.Tensor): The learning label of the prediction.\n",
    "        weight (torch.Tensor, optional): Sample-wise loss weight.\n",
    "        gamma (float, optional): The gamma for calculating the modulating\n",
    "            factor. Defaults to 2.0.\n",
    "        alpha (float, optional): A balanced form for Focal Loss.\n",
    "            Defaults to 0.25.\n",
    "        reduction (str, optional): The method used to reduce the loss into\n",
    "            a scalar. Defaults to 'mean'. Options are \"none\", \"mean\" and \"sum\".\n",
    "        avg_factor (int, optional): Average factor that is used to average\n",
    "            the loss. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Function.apply does not accept keyword arguments, so the decorator\n",
    "    # \"weighted_loss\" is not applicable\n",
    "    from mmcv.ops import sigmoid_focal_loss as _sigmoid_focal_loss\n",
    "    loss = _sigmoid_focal_loss(pred.contiguous(), target.contiguous(), gamma,\n",
    "                               alpha, None, 'none')\n",
    "    if weight is not None:\n",
    "        if weight.shape != loss.shape:\n",
    "            if weight.size(0) == loss.size(0):\n",
    "                # For most cases, weight is of shape (num_priors, ),\n",
    "                #  which means it does not have the second axis num_class\n",
    "                weight = weight.view(-1, 1)\n",
    "            else:\n",
    "                # Sometimes, weight per anchor per class is also needed. e.g.\n",
    "                #  in FSAF. But it may be flattened of shape\n",
    "                #  (num_priors x num_class, ), while loss is still of shape\n",
    "                #  (num_priors, num_class).\n",
    "                assert weight.numel() == loss.numel()\n",
    "                weight = weight.view(loss.size(0), -1)\n",
    "        assert weight.ndim == loss.ndim\n",
    "    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 use_sigmoid=True,\n",
    "                 gamma=2.0,\n",
    "                 alpha=0.25,\n",
    "                 reduction='mean',\n",
    "                 loss_weight=1.0):\n",
    "        \"\"\"`Focal Loss <https://arxiv.org/abs/1708.02002>`_\n",
    "\n",
    "        Args:\n",
    "            use_sigmoid (bool, optional): Whether to the prediction is\n",
    "                used for sigmoid or softmax. Defaults to True.\n",
    "            gamma (float, optional): The gamma for calculating the modulating\n",
    "                factor. Defaults to 2.0.\n",
    "            alpha (float, optional): A balanced form for Focal Loss.\n",
    "                Defaults to 0.25.\n",
    "            reduction (str, optional): The method used to reduce the loss into\n",
    "                a scalar. Defaults to 'mean'. Options are \"none\", \"mean\" and\n",
    "                \"sum\".\n",
    "            loss_weight (float, optional): Weight of loss. Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        assert use_sigmoid is True, 'Only sigmoid focal loss supported now.'\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "    def forward(self,\n",
    "                pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                avg_factor=None,\n",
    "                reduction_override=None):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): The prediction.\n",
    "            target (torch.Tensor): The learning label of the prediction.\n",
    "            weight (torch.Tensor, optional): The weight of loss for each\n",
    "                prediction. Defaults to None.\n",
    "            avg_factor (int, optional): Average factor that is used to average\n",
    "                the loss. Defaults to None.\n",
    "            reduction_override (str, optional): The reduction method used to\n",
    "                override the original reduction method of the loss.\n",
    "                Options are \"none\", \"mean\" and \"sum\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated loss\n",
    "        \"\"\"\n",
    "        assert reduction_override in (None, 'none', 'mean', 'sum')\n",
    "        reduction = (\n",
    "            reduction_override if reduction_override else self.reduction)\n",
    "        if self.use_sigmoid:\n",
    "            if torch.cuda.is_available() and pred.is_cuda:\n",
    "                calculate_loss_func = sigmoid_focal_loss\n",
    "            else:\n",
    "                num_classes = pred.size(1)\n",
    "                target = F.one_hot(target, num_classes=num_classes + 1)\n",
    "                target = target[:, :num_classes]\n",
    "                calculate_loss_func = py_sigmoid_focal_loss\n",
    "\n",
    "            loss_cls = self.loss_weight * calculate_loss_func(\n",
    "                pred,\n",
    "                target,\n",
    "                weight,\n",
    "                gamma=self.gamma,\n",
    "                alpha=self.alpha,\n",
    "                reduction=reduction,\n",
    "                avg_factor=avg_factor)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return loss_cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624ef00-1cb8-41ba-bd61-f6979423d8ea",
   "metadata": {},
   "source": [
    "# BinaryFocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, logits=True, reduce=True):\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, pred_logits, target):\n",
    "        pred_sigmoid = pred_logits.sigmoid()\n",
    "        target = target.type_as(pred_logits)\n",
    "        pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n",
    "        focal_weight = (self.alpha * target + (1 - self.alpha) *\n",
    "                        (1 - target)) * pt.pow(self.gamma)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(pred_logits, target, reduction='none') \n",
    "        loss = loss*focal_weight\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552da96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
