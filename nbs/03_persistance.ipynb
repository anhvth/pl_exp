{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5ec44-d243-46cb-8a98-aa1ce7bd992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|default_exp persistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4ae80-dbb2-416a-8db5-8c7e3fb26ff0",
   "metadata": {},
   "source": [
    "# Persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66061968-da7c-48c9-959c-fc6b1574fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import ctypes\n",
    "import fnmatch\n",
    "import importlib\n",
    "import inspect\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import types\n",
    "import io\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import html\n",
    "import hashlib\n",
    "import glob\n",
    "import tempfile\n",
    "import urllib\n",
    "import urllib.request\n",
    "import uuid\n",
    "\n",
    "from distutils.util import strtobool\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "\n",
    "# Util classes\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class EasyDict(dict):\n",
    "    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        self[name] = value\n",
    "\n",
    "    def __delattr__(self, name: str) -> None:\n",
    "        del self[name]\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    \"\"\"Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file.\"\"\"\n",
    "\n",
    "    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n",
    "        self.file = None\n",
    "\n",
    "        if file_name is not None:\n",
    "            self.file = open(file_name, file_mode)\n",
    "\n",
    "        self.should_flush = should_flush\n",
    "        self.stdout = sys.stdout\n",
    "        self.stderr = sys.stderr\n",
    "\n",
    "        sys.stdout = self\n",
    "        sys.stderr = self\n",
    "\n",
    "    def __enter__(self) -> \"Logger\":\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n",
    "        self.close()\n",
    "\n",
    "    def write(self, text: Union[str, bytes]) -> None:\n",
    "        \"\"\"Write text to stdout (and a file) and optionally flush.\"\"\"\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode()\n",
    "        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n",
    "            return\n",
    "\n",
    "        if self.file is not None:\n",
    "            self.file.write(text)\n",
    "\n",
    "        self.stdout.write(text)\n",
    "\n",
    "        if self.should_flush:\n",
    "            self.flush()\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        \"\"\"Flush written text to both stdout and a file, if open.\"\"\"\n",
    "        if self.file is not None:\n",
    "            self.file.flush()\n",
    "\n",
    "        self.stdout.flush()\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"Flush, close possible files, and remove stdout/stderr mirroring.\"\"\"\n",
    "        self.flush()\n",
    "\n",
    "        # if using multiple loggers, prevent closing in wrong order\n",
    "        if sys.stdout is self:\n",
    "            sys.stdout = self.stdout\n",
    "        if sys.stderr is self:\n",
    "            sys.stderr = self.stderr\n",
    "\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n",
    "            self.file = None\n",
    "\n",
    "\n",
    "# Cache directories\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "_dnnlib_cache_dir = None\n",
    "\n",
    "def set_cache_dir(path: str) -> None:\n",
    "    global _dnnlib_cache_dir\n",
    "    _dnnlib_cache_dir = path\n",
    "\n",
    "def make_cache_dir_path(*paths: str) -> str:\n",
    "    if _dnnlib_cache_dir is not None:\n",
    "        return os.path.join(_dnnlib_cache_dir, *paths)\n",
    "    if 'DNNLIB_CACHE_DIR' in os.environ:\n",
    "        return os.path.join(os.environ['DNNLIB_CACHE_DIR'], *paths)\n",
    "    if 'HOME' in os.environ:\n",
    "        return os.path.join(os.environ['HOME'], '.cache', 'dnnlib', *paths)\n",
    "    if 'USERPROFILE' in os.environ:\n",
    "        return os.path.join(os.environ['USERPROFILE'], '.cache', 'dnnlib', *paths)\n",
    "    return os.path.join(tempfile.gettempdir(), '.cache', 'dnnlib', *paths)\n",
    "\n",
    "# Small util functions\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def format_time(seconds: Union[int, float]) -> str:\n",
    "    \"\"\"Convert the seconds to human readable string with days, hours, minutes and seconds.\"\"\"\n",
    "    s = int(np.rint(seconds))\n",
    "\n",
    "    if s < 60:\n",
    "        return \"{0}s\".format(s)\n",
    "    elif s < 60 * 60:\n",
    "        return \"{0}m {1:02}s\".format(s // 60, s % 60)\n",
    "    elif s < 24 * 60 * 60:\n",
    "        return \"{0}h {1:02}m {2:02}s\".format(s // (60 * 60), (s // 60) % 60, s % 60)\n",
    "    else:\n",
    "        return \"{0}d {1:02}h {2:02}m\".format(s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n",
    "\n",
    "\n",
    "def ask_yes_no(question: str) -> bool:\n",
    "    \"\"\"Ask the user the question until the user inputs a valid answer.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"{0} [y/n]\".format(question))\n",
    "            return strtobool(input().lower())\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def tuple_product(t: Tuple) -> Any:\n",
    "    \"\"\"Calculate the product of the tuple elements.\"\"\"\n",
    "    result = 1\n",
    "\n",
    "    for v in t:\n",
    "        result *= v\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "_str_to_ctype = {\n",
    "    \"uint8\": ctypes.c_ubyte,\n",
    "    \"uint16\": ctypes.c_uint16,\n",
    "    \"uint32\": ctypes.c_uint32,\n",
    "    \"uint64\": ctypes.c_uint64,\n",
    "    \"int8\": ctypes.c_byte,\n",
    "    \"int16\": ctypes.c_int16,\n",
    "    \"int32\": ctypes.c_int32,\n",
    "    \"int64\": ctypes.c_int64,\n",
    "    \"float32\": ctypes.c_float,\n",
    "    \"float64\": ctypes.c_double\n",
    "}\n",
    "\n",
    "\n",
    "def get_dtype_and_ctype(type_obj: Any) -> Tuple[np.dtype, Any]:\n",
    "    \"\"\"Given a type name string (or an object having a __name__ attribute), return matching Numpy and ctypes types that have the same size in bytes.\"\"\"\n",
    "    type_str = None\n",
    "\n",
    "    if isinstance(type_obj, str):\n",
    "        type_str = type_obj\n",
    "    elif hasattr(type_obj, \"__name__\"):\n",
    "        type_str = type_obj.__name__\n",
    "    elif hasattr(type_obj, \"name\"):\n",
    "        type_str = type_obj.name\n",
    "    else:\n",
    "        raise RuntimeError(\"Cannot infer type name from input\")\n",
    "\n",
    "    assert type_str in _str_to_ctype.keys()\n",
    "\n",
    "    my_dtype = np.dtype(type_str)\n",
    "    my_ctype = _str_to_ctype[type_str]\n",
    "\n",
    "    assert my_dtype.itemsize == ctypes.sizeof(my_ctype)\n",
    "\n",
    "    return my_dtype, my_ctype\n",
    "\n",
    "\n",
    "def is_pickleable(obj: Any) -> bool:\n",
    "    try:\n",
    "        with io.BytesIO() as stream:\n",
    "            pickle.dump(obj, stream)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Functionality to import modules/objects by name, and call functions by name\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_module_from_obj_name(obj_name: str) -> Tuple[types.ModuleType, str]:\n",
    "    \"\"\"Searches for the underlying module behind the name to some python object.\n",
    "    Returns the module and the object name (original name with module part removed).\"\"\"\n",
    "\n",
    "    # allow convenience shorthands, substitute them by full names\n",
    "    obj_name = re.sub(\"^np.\", \"numpy.\", obj_name)\n",
    "    obj_name = re.sub(\"^tf.\", \"tensorflow.\", obj_name)\n",
    "\n",
    "    # list alternatives for (module_name, local_obj_name)\n",
    "    parts = obj_name.split(\".\")\n",
    "    name_pairs = [(\".\".join(parts[:i]), \".\".join(parts[i:])) for i in range(len(parts), 0, -1)]\n",
    "\n",
    "    # try each alternative in turn\n",
    "    for module_name, local_obj_name in name_pairs:\n",
    "        try:\n",
    "            module = importlib.import_module(module_name) # may raise ImportError\n",
    "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
    "            return module, local_obj_name\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # maybe some of the modules themselves contain errors?\n",
    "    for module_name, _local_obj_name in name_pairs:\n",
    "        try:\n",
    "            importlib.import_module(module_name) # may raise ImportError\n",
    "        except ImportError:\n",
    "            if not str(sys.exc_info()[1]).startswith(\"No module named '\" + module_name + \"'\"):\n",
    "                raise\n",
    "\n",
    "    # maybe the requested attribute is missing?\n",
    "    for module_name, local_obj_name in name_pairs:\n",
    "        try:\n",
    "            module = importlib.import_module(module_name) # may raise ImportError\n",
    "            get_obj_from_module(module, local_obj_name) # may raise AttributeError\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    # we are out of luck, but we have no idea why\n",
    "    raise ImportError(obj_name)\n",
    "\n",
    "\n",
    "def get_obj_from_module(module: types.ModuleType, obj_name: str) -> Any:\n",
    "    \"\"\"Traverses the object name and returns the last (rightmost) python object.\"\"\"\n",
    "    if obj_name == '':\n",
    "        return module\n",
    "    obj = module\n",
    "    for part in obj_name.split(\".\"):\n",
    "        obj = getattr(obj, part)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def get_obj_by_name(name: str) -> Any:\n",
    "    \"\"\"Finds the python object with the given name.\"\"\"\n",
    "    module, obj_name = get_module_from_obj_name(name)\n",
    "    return get_obj_from_module(module, obj_name)\n",
    "\n",
    "\n",
    "def call_func_by_name(*args, func_name: str = None, **kwargs) -> Any:\n",
    "    \"\"\"Finds the python object with the given name and calls it as a function.\"\"\"\n",
    "    assert func_name is not None\n",
    "    func_obj = get_obj_by_name(func_name)\n",
    "    assert callable(func_obj)\n",
    "    return func_obj(*args, **kwargs)\n",
    "\n",
    "\n",
    "def construct_class_by_name(*args, class_name: str = None, **kwargs) -> Any:\n",
    "    \"\"\"Finds the python class with the given name and constructs it with the given arguments.\"\"\"\n",
    "    return call_func_by_name(*args, func_name=class_name, **kwargs)\n",
    "\n",
    "\n",
    "def get_module_dir_by_obj_name(obj_name: str) -> str:\n",
    "    \"\"\"Get the directory path of the module containing the given object name.\"\"\"\n",
    "    module, _ = get_module_from_obj_name(obj_name)\n",
    "    return os.path.dirname(inspect.getfile(module))\n",
    "\n",
    "\n",
    "def is_top_level_function(obj: Any) -> bool:\n",
    "    \"\"\"Determine whether the given object is a top-level function, i.e., defined at module scope using 'def'.\"\"\"\n",
    "    return callable(obj) and obj.__name__ in sys.modules[obj.__module__].__dict__\n",
    "\n",
    "\n",
    "def get_top_level_function_name(obj: Any) -> str:\n",
    "    \"\"\"Return the fully-qualified name of a top-level function.\"\"\"\n",
    "    assert is_top_level_function(obj)\n",
    "    module = obj.__module__\n",
    "    if module == '__main__':\n",
    "        module = os.path.splitext(os.path.basename(sys.modules[module].__file__))[0]\n",
    "    return module + \".\" + obj.__name__\n",
    "\n",
    "\n",
    "# File system helpers\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "def list_dir_recursively_with_ignore(dir_path: str, ignores: List[str] = None, add_base_to_relative: bool = False) -> List[Tuple[str, str]]:\n",
    "    \"\"\"List all files recursively in a given directory while ignoring given file and directory names.\n",
    "    Returns list of tuples containing both absolute and relative paths.\"\"\"\n",
    "    assert os.path.isdir(dir_path)\n",
    "    base_name = os.path.basename(os.path.normpath(dir_path))\n",
    "\n",
    "    if ignores is None:\n",
    "        ignores = []\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for root, dirs, files in os.walk(dir_path, topdown=True):\n",
    "        for ignore_ in ignores:\n",
    "            dirs_to_remove = [d for d in dirs if fnmatch.fnmatch(d, ignore_)]\n",
    "\n",
    "            # dirs need to be edited in-place\n",
    "            for d in dirs_to_remove:\n",
    "                dirs.remove(d)\n",
    "\n",
    "            files = [f for f in files if not fnmatch.fnmatch(f, ignore_)]\n",
    "\n",
    "        absolute_paths = [os.path.join(root, f) for f in files]\n",
    "        relative_paths = [os.path.relpath(p, dir_path) for p in absolute_paths]\n",
    "\n",
    "        if add_base_to_relative:\n",
    "            relative_paths = [os.path.join(base_name, p) for p in relative_paths]\n",
    "\n",
    "        assert len(absolute_paths) == len(relative_paths)\n",
    "        result += zip(absolute_paths, relative_paths)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def copy_files_and_create_dirs(files: List[Tuple[str, str]]) -> None:\n",
    "    \"\"\"Takes in a list of tuples of (src, dst) paths and copies files.\n",
    "    Will create all necessary directories.\"\"\"\n",
    "    for file in files:\n",
    "        target_dir_name = os.path.dirname(file[1])\n",
    "\n",
    "        # will create all intermediate-level directories\n",
    "        if not os.path.exists(target_dir_name):\n",
    "            os.makedirs(target_dir_name)\n",
    "\n",
    "        shutil.copyfile(file[0], file[1])\n",
    "\n",
    "\n",
    "# URL helpers\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "def is_url(obj: Any, allow_file_urls: bool = False) -> bool:\n",
    "    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n",
    "    if not isinstance(obj, str) or not \"://\" in obj:\n",
    "        return False\n",
    "    if allow_file_urls and obj.startswith('file://'):\n",
    "        return True\n",
    "    try:\n",
    "        res = requests.compat.urlparse(obj)\n",
    "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
    "            return False\n",
    "        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n",
    "        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_filename: bool = False, cache: bool = True) -> Any:\n",
    "    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n",
    "    assert num_attempts >= 1\n",
    "    assert not (return_filename and (not cache))\n",
    "\n",
    "    # Doesn't look like an URL scheme so interpret it as a local filename.\n",
    "    if not re.match('^[a-z]+://', url):\n",
    "        return url if return_filename else open(url, \"rb\")\n",
    "\n",
    "    # Handle file URLs.  This code handles unusual file:// patterns that\n",
    "    # arise on Windows:\n",
    "    #\n",
    "    # file:///c:/foo.txt\n",
    "    #\n",
    "    # which would translate to a local '/c:/foo.txt' filename that's\n",
    "    # invalid.  Drop the forward slash for such pathnames.\n",
    "    #\n",
    "    # If you touch this code path, you should test it on both Linux and\n",
    "    # Windows.\n",
    "    #\n",
    "    # Some internet resources suggest using urllib.request.url2pathname() but\n",
    "    # but that converts forward slashes to backslashes and this causes\n",
    "    # its own set of problems.\n",
    "    if url.startswith('file://'):\n",
    "        filename = urllib.parse.urlparse(url).path\n",
    "        if re.match(r'^/[a-zA-Z]:', filename):\n",
    "            filename = filename[1:]\n",
    "        return filename if return_filename else open(filename, \"rb\")\n",
    "\n",
    "    assert is_url(url)\n",
    "\n",
    "    # Lookup from cache.\n",
    "    if cache_dir is None:\n",
    "        cache_dir = make_cache_dir_path('downloads')\n",
    "\n",
    "    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n",
    "    if cache:\n",
    "        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n",
    "        if len(cache_files) == 1:\n",
    "            filename = cache_files[0]\n",
    "            return filename if return_filename else open(filename, \"rb\")\n",
    "\n",
    "    # Download.\n",
    "    url_name = None\n",
    "    url_data = None\n",
    "    with requests.Session() as session:\n",
    "        if verbose:\n",
    "            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n",
    "        for attempts_left in reversed(range(num_attempts)):\n",
    "            try:\n",
    "                with session.get(url) as res:\n",
    "                    res.raise_for_status()\n",
    "                    if len(res.content) == 0:\n",
    "                        raise IOError(\"No data received\")\n",
    "\n",
    "                    if len(res.content) < 8192:\n",
    "                        content_str = res.content.decode(\"utf-8\")\n",
    "                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n",
    "                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n",
    "                            if len(links) == 1:\n",
    "                                url = requests.compat.urljoin(url, links[0])\n",
    "                                raise IOError(\"Google Drive virus checker nag\")\n",
    "                        if \"Google Drive - Quota exceeded\" in content_str:\n",
    "                            raise IOError(\"Google Drive download quota exceeded -- please try again later\")\n",
    "\n",
    "                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n",
    "                    url_name = match[1] if match else url\n",
    "                    url_data = res.content\n",
    "                    if verbose:\n",
    "                        print(\" done\")\n",
    "                    break\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except:\n",
    "                if not attempts_left:\n",
    "                    if verbose:\n",
    "                        print(\" failed\")\n",
    "                    raise\n",
    "                if verbose:\n",
    "                    print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    # Save to cache.\n",
    "    if cache:\n",
    "        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n",
    "        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n",
    "        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            f.write(url_data)\n",
    "        os.replace(temp_file, cache_file) # atomic\n",
    "        if return_filename:\n",
    "            return cache_file\n",
    "\n",
    "    # Return data as file object.\n",
    "    assert not return_filename\n",
    "    return io.BytesIO(url_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922270d3-7ce6-4fe5-aede-5cc8adabf4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
    "\n",
    "\"\"\"Facilities for pickling Python code alongside other data.\n",
    "\n",
    "The pickled code is automatically imported into a separate Python module\n",
    "during unpickling. This way, any previously exported pickles will remain\n",
    "usable even if the original code is no longer available, or if the current\n",
    "version of the code is not consistent with what was originally pickled.\"\"\"\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import io\n",
    "import inspect\n",
    "import copy\n",
    "import uuid\n",
    "import types\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "_version            = 6         # internal version number\n",
    "_decorators         = set()     # {decorator_class, ...}\n",
    "_import_hooks       = []        # [hook_function, ...]\n",
    "_module_to_src_dict = dict()    # {module: src, ...}\n",
    "_src_to_module_dict = dict()    # {src: module, ...}\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def persistent_class(orig_class):\n",
    "    r\"\"\"Class decorator that extends a given class to save its source code\n",
    "    when pickled.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        from torch_utils import persistence\n",
    "\n",
    "        @persistence.persistent_class\n",
    "        class MyNetwork(torch.nn.Module):\n",
    "            def __init__(self, num_inputs, num_outputs):\n",
    "                super().__init__()\n",
    "                self.fc = MyLayer(num_inputs, num_outputs)\n",
    "                ...\n",
    "\n",
    "        @persistence.persistent_class\n",
    "        class MyLayer(torch.nn.Module):\n",
    "            ...\n",
    "\n",
    "    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n",
    "    source code alongside other internal state (e.g., parameters, buffers,\n",
    "    and submodules). This way, any previously exported pickle will remain\n",
    "    usable even if the class definitions have been modified or are no\n",
    "    longer available.\n",
    "\n",
    "    The decorator saves the source code of the entire Python module\n",
    "    containing the decorated class. It does *not* save the source code of\n",
    "    any imported modules. Thus, the imported modules must be available\n",
    "    during unpickling, also including `torch_utils.persistence` itself.\n",
    "\n",
    "    It is ok to call functions defined in the same module from the\n",
    "    decorated class. However, if the decorated class depends on other\n",
    "    classes defined in the same module, they must be decorated as well.\n",
    "    This is illustrated in the above example in the case of `MyLayer`.\n",
    "\n",
    "    It is also possible to employ the decorator just-in-time before\n",
    "    calling the constructor. For example:\n",
    "\n",
    "        cls = MyLayer\n",
    "        if want_to_make_it_persistent:\n",
    "            cls = persistence.persistent_class(cls)\n",
    "        layer = cls(num_inputs, num_outputs)\n",
    "\n",
    "    As an additional feature, the decorator also keeps track of the\n",
    "    arguments that were used to construct each instance of the decorated\n",
    "    class. The arguments can be queried via `obj.init_args` and\n",
    "    `obj.init_kwargs`, and they are automatically pickled alongside other\n",
    "    object state. A typical use case is to first unpickle a previous\n",
    "    instance of a persistent class, and then upgrade it to use the latest\n",
    "    version of the source code:\n",
    "\n",
    "        with open('old_pickle.pkl', 'rb') as f:\n",
    "            old_net = pickle.load(f)\n",
    "        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n",
    "        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n",
    "    \"\"\"\n",
    "    assert isinstance(orig_class, type)\n",
    "    if is_persistent(orig_class):\n",
    "        return orig_class\n",
    "\n",
    "    assert orig_class.__module__ in sys.modules\n",
    "    orig_module = sys.modules[orig_class.__module__]\n",
    "    orig_module_src = _module_to_src(orig_module)\n",
    "\n",
    "    class Decorator(orig_class):\n",
    "        _orig_module_src = orig_module_src\n",
    "        _orig_class_name = orig_class.__name__\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self._init_args = copy.deepcopy(args)\n",
    "            self._init_kwargs = copy.deepcopy(kwargs)\n",
    "            assert orig_class.__name__ in orig_module.__dict__\n",
    "            _check_pickleable(self.__reduce__())\n",
    "\n",
    "        @property\n",
    "        def init_args(self):\n",
    "            return copy.deepcopy(self._init_args)\n",
    "\n",
    "        @property\n",
    "        def init_kwargs(self):\n",
    "            return EasyDict(copy.deepcopy(self._init_kwargs))\n",
    "\n",
    "        def __reduce__(self):\n",
    "            fields = list(super().__reduce__())\n",
    "            fields += [None] * max(3 - len(fields), 0)\n",
    "            if fields[0] is not _reconstruct_persistent_obj:\n",
    "                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n",
    "                fields[0] = _reconstruct_persistent_obj # reconstruct func\n",
    "                fields[1] = (meta,) # reconstruct args\n",
    "                fields[2] = None # state dict\n",
    "            return tuple(fields)\n",
    "\n",
    "    Decorator.__name__ = orig_class.__name__\n",
    "    _decorators.add(Decorator)\n",
    "    return Decorator\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def is_persistent(obj):\n",
    "    r\"\"\"Test whether the given object or class is persistent, i.e.,\n",
    "    whether it will save its source code when pickled.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if obj in _decorators:\n",
    "            return True\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def import_hook(hook):\n",
    "    r\"\"\"Register an import hook that is called whenever a persistent object\n",
    "    is being unpickled. A typical use case is to patch the pickled source\n",
    "    code to avoid errors and inconsistencies when the API of some imported\n",
    "    module has changed.\n",
    "\n",
    "    The hook should have the following signature:\n",
    "\n",
    "        hook(meta) -> modified meta\n",
    "\n",
    "    `meta` is an instance of `EasyDict` with the following fields:\n",
    "\n",
    "        type:       Type of the persistent object, e.g. `'class'`.\n",
    "        version:    Internal version number of `torch_utils.persistence`.\n",
    "        module_src  Original source code of the Python module.\n",
    "        class_name: Class name in the original Python module.\n",
    "        state:      Internal state of the object.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        @persistence.import_hook\n",
    "        def wreck_my_network(meta):\n",
    "            if meta.class_name == 'MyNetwork':\n",
    "                print('MyNetwork is being imported. I will wreck it!')\n",
    "                meta.module_src = meta.module_src.replace(\"True\", \"False\")\n",
    "            return meta\n",
    "    \"\"\"\n",
    "    assert callable(hook)\n",
    "    _import_hooks.append(hook)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def _reconstruct_persistent_obj(meta):\n",
    "    r\"\"\"Hook that is called internally by the `pickle` module to unpickle\n",
    "    a persistent object.\n",
    "    \"\"\"\n",
    "    meta = EasyDict(meta)\n",
    "    meta.state = EasyDict(meta.state)\n",
    "    for hook in _import_hooks:\n",
    "        meta = hook(meta)\n",
    "        assert meta is not None\n",
    "\n",
    "    assert meta.version == _version\n",
    "    module = _src_to_module(meta.module_src)\n",
    "\n",
    "    assert meta.type == 'class'\n",
    "    orig_class = module.__dict__[meta.class_name]\n",
    "    decorator_class = persistent_class(orig_class)\n",
    "    obj = decorator_class.__new__(decorator_class)\n",
    "\n",
    "    setstate = getattr(obj, '__setstate__', None)\n",
    "    if callable(setstate):\n",
    "        setstate(meta.state) # pylint: disable=not-callable\n",
    "    else:\n",
    "        obj.__dict__.update(meta.state)\n",
    "    return obj\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def _module_to_src(module):\n",
    "    r\"\"\"Query the source code of a given Python module.\n",
    "    \"\"\"\n",
    "    src = _module_to_src_dict.get(module, None)\n",
    "    if src is None:\n",
    "        src = inspect.getsource(module)\n",
    "        _module_to_src_dict[module] = src\n",
    "        _src_to_module_dict[src] = module\n",
    "    return src\n",
    "\n",
    "def _src_to_module(src):\n",
    "    r\"\"\"Get or create a Python module for the given source code.\n",
    "    \"\"\"\n",
    "    module = _src_to_module_dict.get(src, None)\n",
    "    if module is None:\n",
    "        module_name = \"_imported_module_\" + uuid.uuid4().hex\n",
    "        module = types.ModuleType(module_name)\n",
    "        sys.modules[module_name] = module\n",
    "        _module_to_src_dict[module] = src\n",
    "        _src_to_module_dict[src] = module\n",
    "        exec(src, module.__dict__) # pylint: disable=exec-used\n",
    "    return module\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def _check_pickleable(obj):\n",
    "    r\"\"\"Check that the given object is pickleable, raising an exception if\n",
    "    it is not. This function is expected to be considerably more efficient\n",
    "    than actually pickling the object.\n",
    "    \"\"\"\n",
    "    def recurse(obj):\n",
    "        if isinstance(obj, (list, tuple, set)):\n",
    "            return [recurse(x) for x in obj]\n",
    "        if isinstance(obj, dict):\n",
    "            return [[recurse(x), recurse(y)] for x, y in obj.items()]\n",
    "        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):\n",
    "            return None # Python primitive types are pickleable.\n",
    "        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor']:\n",
    "            return None # NumPy arrays and PyTorch tensors are pickleable.\n",
    "        if is_persistent(obj):\n",
    "            return None # Persistent objects are pickleable, by virtue of the constructor check.\n",
    "        return obj\n",
    "    with io.BytesIO() as f:\n",
    "        pickle.dump(recurse(obj), f)\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c315f-3c27-417f-9bf0-caa1eae29393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_base_model.ipynb.\n",
      "Converted 01_lit_model.ipynb.\n",
      "Converted 02_loss.ipynb.\n",
      "Converted 03_persistance.ipynb.\n",
      "Converted init.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4584f15-29f0-4a30-854a-ff8a47d47651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
