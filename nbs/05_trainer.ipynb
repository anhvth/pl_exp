{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744c89c-dc3d-448a-9471-e01065b025df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74841b7-a593-471e-85ac-197f288db31b",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "> Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20738f-b92f-4990-9a3b-510917be9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import os.path as osp\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "\n",
    "def get_trainer(exp_name, max_epochs, \n",
    "                gpus=1,\n",
    "                monitor=dict(metric=\"val_acc\", mode=\"max\"), \n",
    "                save_every_n_epochs=1, \n",
    "                save_top_k=1, \n",
    "                use_version=True, \n",
    "                strategy=None,\n",
    "                refresh_rate=5):\n",
    "    \"\"\" Example:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    rld = osp.join(\"lightning_logs\", exp_name)\n",
    "    cur_num_exps = len(os.listdir(rld)) if osp.exists(rld) else 0\n",
    "    version = f\"{cur_num_exps:02d}\"\n",
    "    filename = \"{epoch}-{\"+monitor[\"metric\"]+\":.2f}\"\n",
    "    root_log_dir = osp.join(\"lightning_logs\", exp_name, version)\n",
    "    logger.info(f'Log root dir: {root_log_dir}')\n",
    "\n",
    "    callback_ckpt = ModelCheckpoint(\n",
    "        dirpath=osp.join(root_log_dir, \"ckpts\"),\n",
    "        monitor=monitor['metric'], mode=monitor['mode'],\n",
    "        filename=filename,\n",
    "        save_last=True,\n",
    "        every_n_epochs=save_every_n_epochs,\n",
    "        save_top_k=save_top_k,\n",
    "    )\n",
    "\n",
    "    callback_tqdm = TQDMProgressBar(refresh_rate=5)\n",
    "    callback_lrmornitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    \n",
    "    plt_logger = TensorBoardLogger(\n",
    "        osp.join(root_log_dir, \"tb_logs\"),\n",
    "    )\n",
    "    \n",
    "    if strategy is None:\n",
    "        strategy=\"dp\" if gpus < 2 else \"ddp\"\n",
    "    trainer = Trainer(\n",
    "        accelerator ='gpu',\n",
    "        devices=gpus,\n",
    "        max_epochs=max_epochs,\n",
    "        strategy=strategy,\n",
    "        callbacks=[callback_ckpt, callback_tqdm, callback_lrmornitor],\n",
    "        logger=plt_logger,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc466513-20c4-4903-995a-84bd96948c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 03:23:28.872 | INFO     | __main__:get_trainer:29 - Log root dir: lightning_logs/test/00\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "get_trainer('test', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01e273-38e2-46eb-bae8-44e281745429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import os.path as osp\n",
    "# # from argparse import ArgumentParser\n",
    "# from pytorch_lightning import Trainer, seed_everything\n",
    "# # from ple.all import get_trainer, BaseExp\n",
    "import shutil\n",
    "from fastcore.script import *\n",
    "from fastcore.utils import *\n",
    "from loguru import logger\n",
    "\n",
    "def get_rank() -> int:\n",
    "    import torch.distributed as dist\n",
    "    if not dist.is_available():\n",
    "        return 0\n",
    "    if not dist.is_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "def get_exp_by_file(exp_file):\n",
    "    \"\"\"\n",
    "        Params:\n",
    "        exp_file: Path to exp\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        import importlib\n",
    "        import os\n",
    "        import sys\n",
    "        sys.path.append(os.path.dirname(exp_file))\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        current_exp = importlib.import_module(\n",
    "            os.path.basename(exp_file).split(\".\")[0])\n",
    "        current_exp = importlib.reload(current_exp)\n",
    "        exp = current_exp.Exp()\n",
    "        return exp\n",
    "    except Exception:\n",
    "        raise ImportError(\n",
    "            \"{} doesn't contains class named 'Exp'\".format(exp_file))\n",
    "\n",
    "        \n",
    "@call_parse\n",
    "def train(\n",
    "    cfg_path:Param('Path to config'),\n",
    "    devices: Param('GPUS indices', default=1, type=int),\n",
    "    accelerator: Param('cpu or gpu', default='gpu', type=str),\n",
    "    opts: Param('Additional configs', default='', type=str, required=False),\n",
    "):\n",
    "    cfg = get_exp_by_file(cfg_path)\n",
    "    if len(opts):\n",
    "        cfg.merge(opts.replace('=',' ').split(' '))\n",
    "    cfg.devices = devices\n",
    "    \n",
    "    data = cfg.get_data_loader()\n",
    "\n",
    "    model = cfg.get_model(create_lr_scheduler_fn=cfg.get_lr_scheduler(), \n",
    "            create_optimizer_fn=cfg.get_optimizer())\n",
    "    trainer = cfg.get_trainer(devices)\n",
    "    try:\n",
    "        trainer.fit(model, data)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if get_rank() == 0:\n",
    "            out_path = osp.join(trainer.log_dir, osp.basename(cfg_path))\n",
    "            logger.info('cp {} {}', cfg_path, out_path)\n",
    "            shutil.copy(cfg_path, out_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620169c3-678f-414d-81fd-7df0c1f5782c",
   "metadata": {},
   "source": [
    "# Helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb975b4e-5cae-4289-92d9-f7f294e5df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    create_schedule_fn = fn_schedule_cosine_with_warmpup_decay_timm(\n",
      "        num_epochs=EPOCHS,\n",
      "        num_steps_per_epoch=len(dl_train),\n",
      "        num_epochs_per_cycle=EPOCHS//2,\n",
      "        min_lr=1/100,\n",
      "        cycle_decay=0.7,\n",
      "    )\n",
      "\n",
      "    model = Model()\n",
      "    lit_lstm = LitLSTM(model,create_optimizer_fn=lambda params:torch.optim.Adam(params),\n",
      "                                   create_lr_scheduler_fn=create_schedule_fn, loss_fn=nn.L1Loss()\n",
      "                )\n",
      "    trainer = get_trainer('lstm_trainer', EPOCHS, \n",
      "                             monitor={'metric': 'val_loss', 'mode': 'min'},\n",
      "                         )\n"
     ]
    }
   ],
   "source": [
    "example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5162854-0bfd-49e9-a613-64b126b42d06",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff0fe8-b317-4958-a4d3-eb3c6a4c8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_trainer('test', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a2af8-3bb2-4e86-b48a-171ceacef84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ee660-f5b1-4316-9ac9-6969ab37ba67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf7681-8eb7-4be3-bdcf-e71c96f580f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e1a56-fdea-4501-a51e-2f2c8d8ee462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
